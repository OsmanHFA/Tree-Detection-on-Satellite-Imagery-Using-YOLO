{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e42e51-2025-42b0-91b6-5ded50586e12",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f344352-2ba8-4cfd-99ba-d04f89a891a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/ee_tree_counting/Models/SSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b674f43-914b-4602-8922-1f9943816425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import (\n",
    "    DEVICE, \n",
    "    NUM_CLASSES, \n",
    "    NUM_EPOCHS, \n",
    "    OUT_DIR,\n",
    "    VISUALIZE_TRANSFORMED_IMAGES, \n",
    "    NUM_WORKERS,\n",
    "    RESIZE_TO,\n",
    "    VALID_DIR,\n",
    "    TRAIN_DIR,\n",
    "    WEIGHTS_PATH\n",
    ")\n",
    "from model import create_model\n",
    "from custom_utils import (\n",
    "    Averager, \n",
    "    SaveBestModel, \n",
    "    save_model, \n",
    "    save_loss_plot,\n",
    "    save_mAP\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import (\n",
    "    create_train_dataset, \n",
    "    create_valid_dataset, \n",
    "    create_train_loader, \n",
    "    create_valid_loader\n",
    ")\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3344f4-c098-4709-b331-f52b2844dcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for running training iterations.\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        train_loss_hist.send(loss_value)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da0176c-8c5b-4ab8-81a0-f99f002c64c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize tqdm progress bar.\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    target = []\n",
    "    preds = []\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "        # For mAP calculation using Torchmetrics.\n",
    "        #####################################\n",
    "        for i in range(len(images)):\n",
    "            true_dict = dict()\n",
    "            preds_dict = dict()\n",
    "            true_dict['boxes'] = targets[i]['boxes'].detach().cpu()\n",
    "            true_dict['labels'] = targets[i]['labels'].detach().cpu()\n",
    "            preds_dict['boxes'] = outputs[i]['boxes'].detach().cpu()\n",
    "            preds_dict['scores'] = outputs[i]['scores'].detach().cpu()\n",
    "            preds_dict['labels'] = outputs[i]['labels'].detach().cpu()\n",
    "            preds.append(preds_dict)\n",
    "            target.append(true_dict)\n",
    "        #####################################\n",
    "\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(preds, target)\n",
    "    metric_summary = metric.compute()\n",
    "    return metric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047f6da-abe9-45a2-b7ac-ab17e4f0548f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)\n",
    "train_dataset = create_train_dataset(TRAIN_DIR)\n",
    "valid_dataset = create_valid_dataset(VALID_DIR)\n",
    "train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
    "valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "# Initialize the model and move to the computation device.\n",
    "model = create_model(num_classes=NUM_CLASSES, size=RESIZE_TO, weights_path=WEIGHTS_PATH)\n",
    "model = model.to(DEVICE)\n",
    "# print(model)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, nesterov=True)\n",
    "scheduler = StepLR(\n",
    "    optimizer=optimizer, step_size=15, gamma=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# To monitor training loss\n",
    "train_loss_hist = Averager()\n",
    "# To store training loss and mAP values.\n",
    "train_loss_list = []\n",
    "map_50_list = []\n",
    "map_list = []\n",
    "\n",
    "# Mame to save the trained model with.\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# Whether to show transformed images from data loader or not.\n",
    "if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "    from custom_utils import show_tranformed_image\n",
    "    show_tranformed_image(train_loader)\n",
    "\n",
    "# To save best model.\n",
    "save_best_model = SaveBestModel()\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "    # Reset the training loss histories for the current epoch.\n",
    "    train_loss_hist.reset()\n",
    "\n",
    "    # Start timer and carry out training and validation.\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    metric_summary = validate(valid_loader, model)\n",
    "    print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch+1} mAP: {metric_summary['map']}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    map_50_list.append(metric_summary['map_50'])\n",
    "    map_list.append(metric_summary['map'])\n",
    "\n",
    "    # save the best model till now.\n",
    "    save_best_model(\n",
    "        model, float(metric_summary['map']), epoch, 'outputs'\n",
    "    )\n",
    "    # Save the current epoch model.\n",
    "    save_model(epoch, model, optimizer)\n",
    "\n",
    "    # Save loss plot.\n",
    "    save_loss_plot(OUT_DIR, train_loss_list)\n",
    "\n",
    "    # Save mAP plot.\n",
    "    save_mAP(OUT_DIR, map_50_list, map_list)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a113e0-807d-4656-ba0b-55665cc0eceb",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df00948-3466-4184-bc28-3b31e48a8e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00bf4fdb9f94be08dc05f8d53aa3801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP_50: 65.223\n",
      "mAP_50_95: 25.701\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES, size=640, weights_path=WEIGHTS_PATH)\n",
    "checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "test_dataset = create_valid_dataset(\n",
    "    '/home/jupyter/ee_tree_counting/Data/Combined Dataset XML No Aug/test'\n",
    ")\n",
    "test_loader = create_valid_loader(test_dataset, num_workers=NUM_WORKERS)\n",
    "\n",
    "metric_summary = validate(test_loader, model)\n",
    "print(f\"mAP_50: {metric_summary['map_50']*100:.3f}\")\n",
    "print(f\"mAP_50_95: {metric_summary['map']*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cdd9c-caa3-4389-b969-12dd13eb9ce9",
   "metadata": {},
   "source": [
    "# Exporting to CSV file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16bd7ac6-7cc4-4fd8-b436-5e9d94512e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, 101)),\n",
    "    \"Train Loss\": [\n",
    "        4.938, 4.075, 3.83, 3.696, 3.61, 3.531, 3.463, 3.412, 3.359, 3.322, 3.264, 3.22, 3.179, 3.136, 3.104, 3.063,\n",
    "        3.073, 3.057, 3.058, 3.058, 3.049, 3.041, 3.036, 3.03, 3.04, 3.034, 3.017, 3.03, 3.026, 3.014, 3.018, 3.014,\n",
    "        3.012, 3.017, 3.005, 3.008, 3.015, 3.009, 3.008, 3.003, 3.009, 2.999, 3.02, 3.014, 3.011, 3.007, 3.009, 3.006,\n",
    "        3.005, 3.008, 3.013, 3.011, 3.004, 3.007, 3.013, 3.012, 3.007, 3.007, 3.003, 3.002, 3.014, 3.014, 3.012, 3.01,\n",
    "        3.007, 3.009, 3.006, 3.004, 3.011, 3.001, 3.012, 3.01, 3.007, 3.015, 3.009, 3.004, 3.013, 3.007, 3.004, 3.01,\n",
    "        3.012, 3.007, 3.009, 3.009, 3.007, 3.017, 3.01, 3.004, 3.013, 3.007, 3.014, 3.009, 3.006, 3.007, 3.008, 3.01,\n",
    "        3.008, 3.007, 3.006, 3.008\n",
    "    ],\n",
    "    \"mAP\": [\n",
    "        0.0020383174996823072, 0.022861845791339874, 0.08308479189872742, 0.14638210833072662, 0.18756431341171265,\n",
    "        0.20833797752857208, 0.2192489504814148, 0.22915536165237427, 0.23228442668914795, 0.24335764348506927,\n",
    "        0.24488072097301483, 0.24967779219150543, 0.25477665662765503, 0.2594020962715149, 0.2659323215484619,\n",
    "        0.262830913066864, 0.26334813237190247, 0.26300138235092163, 0.2632383406162262, 0.2628127336502075,\n",
    "        0.2647073566913605, 0.2639809250831604, 0.2655472755432129, 0.2646346390247345, 0.2666299641132355,\n",
    "        0.2656112015247345, 0.2669174373149872, 0.26502883434295654, 0.2670244872570038, 0.26789408922195435,\n",
    "        0.2672414183616638, 0.26779478788375854, 0.2676714062690735, 0.26808544993400574, 0.2677479684352875,\n",
    "        0.2680380940437317, 0.26814815402030945, 0.2681480348110199, 0.2679618000984192, 0.2680222690105438,\n",
    "        0.2683304250240326, 0.2683519721031189, 0.2682887315750122, 0.2683258354663849, 0.2684342861175537,\n",
    "        0.2684059739112854, 0.2683124840259552, 0.2683514654636383, 0.26834672689437866, 0.2683456838130951,\n",
    "        0.26834726333618164, 0.2683553993701935, 0.26837506890296936, 0.26835018396377563, 0.26836878061294556,\n",
    "        0.2683883309364319, 0.268428236246109, 0.2684257924556732, 0.2681969106197357, 0.2683669328689575,\n",
    "        0.2683667540550232, 0.2683667540550232, 0.2683671712875366, 0.2683606445789337, 0.26836055517196655,\n",
    "        0.2683664560317993, 0.26836705207824707, 0.2683640420436859, 0.26836854219436646, 0.2683689296245575,\n",
    "        0.2683689296245575, 0.2683689296245575, 0.26836901903152466, 0.2683681845664978, 0.2683696448802948,\n",
    "        0.2683696448802948, 0.2683696448802948, 0.2683698832988739, 0.2683696448802948, 0.2683698832988739,\n",
    "        0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739,\n",
    "        0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739,\n",
    "        0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739,\n",
    "        0.26836860179901123, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739, 0.2683698832988739\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "\n",
    "df.to_csv('training_log_100eNoAug.csv', index=False)\n",
    "\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e6672-c711-4a54-aa2f-3c11e1128726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
